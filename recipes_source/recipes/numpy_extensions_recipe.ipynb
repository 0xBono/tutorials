{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "numpy_extensions_recipe.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjhSh-86CvG9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL94U-qNCvHC",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Creating Extensions Using numpy and scipy\n",
        "\n",
        "Numpy and scipy are a some of the most popular open source libraries for scientific computing. Because of PyTorch' pythonic nature, users can easily leverage the broader ecosystem of python libaries. In this recipe, we will show how you can leverage these libararies along with PyTorch to:\n",
        "\n",
        "1. Create a neural network layer with no parameters using **numpy**; and \n",
        "\n",
        "2. Create a neural network layer that has learnable weights using **SciPy**\n",
        "\n",
        "**Original Author**: [Adam Paszke](https://github.com/apaszke)\n",
        "\n",
        "**Updated by**: [Adam Dziedzic](https://github.com/adam-dziedzic) and [Joe Spisak](https://github.com/jspisak)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiX2OYUpEpZg",
        "colab_type": "text"
      },
      "source": [
        "### Let's first import the basics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM8ujQylCvHD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6e0IP_PCvHG",
        "colab_type": "text"
      },
      "source": [
        "## Now let's create a parameter-less example of a layer using numpy\n",
        "\n",
        "This layer doesnâ€™t particularly do anything useful or mathematically\n",
        "correct.\n",
        "\n",
        "It is aptly named BadFFTFunction\n",
        "\n",
        "**Layer Implementation**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqSAH1UkCvHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.fft import rfft2, irfft2\n",
        "\n",
        "\n",
        "class BadFFTFunction(Function):\n",
        "\n",
        "    def forward(self, input):\n",
        "        numpy_input = input.detach().numpy()\n",
        "        result = abs(rfft2(numpy_input))\n",
        "        return input.new(result)\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        numpy_go = grad_output.numpy()\n",
        "        result = irfft2(numpy_go)\n",
        "        return grad_output.new(result)\n",
        "\n",
        "# since this layer does not have any parameters, we can\n",
        "# simply declare this as a function, rather than as an nn.Module class\n",
        "\n",
        "\n",
        "def incorrect_fft(input):\n",
        "    return BadFFTFunction()(input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNvw2q1ACvHK",
        "colab_type": "text"
      },
      "source": [
        "**Example usage of the created layer:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo1CvWIaCvHK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input = torch.randn(8, 8, requires_grad=True)\n",
        "result = incorrect_fft(input)\n",
        "print(result)\n",
        "result.backward(torch.randn(result.size()))\n",
        "print(input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj2-qAJCCvHN",
        "colab_type": "text"
      },
      "source": [
        "## Next let's create a parametrized example using scipy\n",
        "\n",
        "In deep learning literature, this layer is confusingly referred\n",
        "to as convolution while the actual operation is cross-correlation\n",
        "(the only difference is that filter is flipped for convolution, which is not the case for cross-correlation).\n",
        "\n",
        "Below is the implementation of a layer with learnable weights, where cross-correlation has a filter (kernel) that represents the weights.\n",
        "\n",
        "The backward pass computes the gradient with respect to the input and the gradient with respect to the filter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YW6YZ26HCvHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import flip\n",
        "import numpy as np\n",
        "from scipy.signal import convolve2d, correlate2d\n",
        "from torch.nn.modules.module import Module\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "\n",
        "class ScipyConv2dFunction(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, filter, bias):\n",
        "        # detach so we can cast to NumPy\n",
        "        input, filter, bias = input.detach(), filter.detach(), bias.detach()\n",
        "        result = correlate2d(input.numpy(), filter.numpy(), mode='valid')\n",
        "        result += bias.numpy()\n",
        "        ctx.save_for_backward(input, filter, bias)\n",
        "        return torch.as_tensor(result, dtype=input.dtype)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        grad_output = grad_output.detach()\n",
        "        input, filter, bias = ctx.saved_tensors\n",
        "        grad_output = grad_output.numpy()\n",
        "        grad_bias = np.sum(grad_output, keepdims=True)\n",
        "        grad_input = convolve2d(grad_output, filter.numpy(), mode='full')\n",
        "        # the previous line can be expressed equivalently as:\n",
        "        # grad_input = correlate2d(grad_output, flip(flip(filter.numpy(), axis=0), axis=1), mode='full')\n",
        "        grad_filter = correlate2d(input.numpy(), grad_output, mode='valid')\n",
        "        return torch.from_numpy(grad_input), torch.from_numpy(grad_filter).to(torch.float), torch.from_numpy(grad_bias).to(torch.float)\n",
        "\n",
        "\n",
        "class ScipyConv2d(Module):\n",
        "    def __init__(self, filter_width, filter_height):\n",
        "        super(ScipyConv2d, self).__init__()\n",
        "        self.filter = Parameter(torch.randn(filter_width, filter_height))\n",
        "        self.bias = Parameter(torch.randn(1, 1))\n",
        "\n",
        "    def forward(self, input):\n",
        "        return ScipyConv2dFunction.apply(input, self.filter, self.bias)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "094F0pWVCvHQ",
        "colab_type": "text"
      },
      "source": [
        "**Example usage:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzP0famSCvHR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "module = ScipyConv2d(3, 3)\n",
        "print(\"Filter and bias: \", list(module.parameters()))\n",
        "input = torch.randn(10, 10, requires_grad=True)\n",
        "output = module(input)\n",
        "print(\"Output from the convolution: \", output)\n",
        "output.backward(torch.randn(8, 8))\n",
        "print(\"Gradient for the input map: \", input.grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDcCnAiLCvHU",
        "colab_type": "text"
      },
      "source": [
        "**Check the gradients:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWBxY_MHCvHU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.autograd.gradcheck import gradcheck\n",
        "\n",
        "moduleConv = ScipyConv2d(3, 3)\n",
        "\n",
        "input = [torch.randn(20, 20, dtype=torch.double, requires_grad=True)]\n",
        "test = gradcheck(moduleConv, input, eps=1e-6, atol=1e-4)\n",
        "print(\"Are the gradients correct: \", test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWgn4uh1FyGx",
        "colab_type": "text"
      },
      "source": [
        "Congrats! You've learned how to create extensions to PyTorch using numpy and scipy. We recommend trying to generalize these approaches to new areas and problems. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agyyA9_yHVGS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}