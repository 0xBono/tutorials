.. note::
    :class: sphx-glr-download-link-note

    Click :ref:`here <sphx_glr_download_recipes_recipes_custom_dataset_transforms_loader.py>` to download the full example code
.. rst-class:: sphx-glr-example-title

.. _sphx_glr_recipes_recipes_custom_dataset_transforms_loader.py:


Developing Custom PyTorch Dataloaders
=====================================

A significant amount of the effort applied to developing machine
learning algorithms is related to data preparation. PyTorch provides
many tools to make data loading easy and hopefully, makes your code more
readable. In this recipe, you will learn how to:

 1. Create a custom dataset leveraging the PyTorch dataset APIs;
 2. Create callable custom transforms that can be composable; and
 3. Put these components together to create a custom dataloader.

Please note, to run this tutorial, ensure the following packages are
installed:
 -  ``scikit-image``: For image io and transforms
 -  ``pandas``: For easier csv parsing

As a point of attribution, this recipe is based on the original tutorial
from `Sasank Chilamkurthy <https://chsasank.github.io>`__ and was later
edited by `Joe Spisak <https://github.com/jspisak>`__.

Setup
----------------------
First let’s import all of the needed libraries for this recipe.




.. code-block:: default












    # Ignore warnings







Part 1: The Dataset
-------------------


The dataset we are going to deal with is that of facial pose. Overall,
68 different landmark points are annotated for each face.

As a next step, please download the dataset from
`here <https://download.pytorch.org/tutorial/faces.zip>`_ so that the
images are in a directory named ‘data/faces/’.

**Note:** This dataset was actually generated by applying
`dlib's pose estimation <https://blog.dlib.net/2014/08/real-time-face-pose-estimation.html>`_
on images from the imagenet dataset containing the ‘face’ tag.

::

   !wget https://download.pytorch.org/tutorial/faces.zip
   !mkdir data/faces/
   import zipfile
   with zipfile.ZipFile("faces.zip","r") as zip_ref:
   zip_ref.extractall("/data/faces/")
   %cd /data/faces/

The dataset comes with a csv file with annotations which looks like
this:

::

     image_name,part_0_x,part_0_y,part_1_x,part_1_y,part_2_x, ... ,part_67_x,part_67_y
     0805personali01.jpg,27,83,27,98, ... 84,134
     1084239450_e76e00b7e7.jpg,70,236,71,257, ... ,128,312

Let’s quickly read the CSV and get the annotations in an (N, 2) array
where N is the number of landmarks.


1.1 Write a simple helper function to show an image
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Next let’s write a simple helper function to show an image, its landmarks and use it to show a sample.



1.2 Create a dataset class
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Now lets talk about the PyTorch dataset class



``torch.utils.data.Dataset`` is an abstract class representing a
dataset. Your custom dataset should inherit ``Dataset`` and override the
following methods:

-  ``__len__`` so that ``len(dataset)`` returns the size of the dataset.
-  ``__getitem__`` to support indexing such that ``dataset[i]`` can be
   used to get :math:``i`` th sample

Let’s create a dataset class for our face landmarks dataset. We will
read the csv in ``__init__`` but leave the reading of images to
``__getitem__``. This is memory efficient because all the images are not
stored in the memory at once but read as required.

Here we show a sample of our dataset in the forma of a dict
``{'image': image, 'landmarks': landmarks}``. Our dataset will take an
optional argument ``transform`` so that any required processing can be
applied on the sample. We will see the usefulness of ``transform`` in
another recipe.


1.3 Iterate through data samples
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


Next let’s instantiate this class and iterate through the data samples.
We will print the sizes of first 4 samples and show their landmarks.


Part 2: Data Tranformations
---------------------------


Now that we have a dataset to work with and have done some level of
customization, we can move to creating custom transformations. In
computer vision, these come in handy to help generalize algorithms and
improve accuracy. A suite of transformations used at training time is
typically referred to as data augmentation and is a common practice for
modern model development.

One issue common in handling datasets is that the samples may not all be
the same size. Most neural networks expect the images of a fixed size.
Therefore, we will need to write some prepocessing code. Let’s create
three transforms:

-  ``Rescale``: to scale the image
-  ``RandomCrop``: to crop from image randomly. This is data
   augmentation.
-  ``ToTensor``: to convert the numpy images to torch images (we need to
   swap axes).

We will write them as callable classes instead of simple functions so
that parameters of the transform need not be passed everytime it’s
called. For this, we just need to implement ``__call__`` method and if
required, ``__init__`` method. We can then use a transform like this:

::

   tsfm = Transform(params)
   transformed_sample = tsfm(sample)

Observe below how these transforms had to be applied both on the image
and landmarks.


2.1 Create callable classes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Let’s start with creating callable classes for each transform



2.2 Compose transforms and apply to a sample
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Next let’s compose these transforms and apply to a sample


Let’s say we want to rescale the shorter side of the image to 256 and
then randomly crop a square of size 224 from it. i.e, we want to compose
``Rescale`` and ``RandomCrop`` transforms.
``torchvision.transforms.Compose`` is a simple callable class which
allows us to do this.



.. code-block:: default







    # Apply each of the above transforms on sample.














2.3 Iterate through the dataset
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Next we will iterate through the dataset


Let’s put this all together to create a dataset with composed
transforms. To summarize, every time this dataset is sampled:

-  An image is read from the file on the fly
-  Transforms are applied on the read image
-  Since one of the transforms is random, data is augmentated on
   sampling

We can iterate over the created dataset with a ``for i in range`` loop
as before.


Part 3: The Dataloader
----------------------


By operating on the dataset directly, we are losing out on a lot of
features by using a simple ``for`` loop to iterate over the data. In
particular, we are missing out on:

-  Batching the data
-  Shuffling the data
-  Load the data in parallel using ``multiprocessing`` workers.

``torch.utils.data.DataLoader`` is an iterator which provides all these
features. Parameters used below should be clear. One parameter of
interest is ``collate_fn``. You can specify how exactly the samples need
to be batched using ``collate_fn``. However, default collate should work
fine for most use cases.



.. code-block:: default






    # Helper function to show a batch
































Now that you’ve learned how to create a custom dataloader with PyTorch,
we recommend diving deeper into the docs and customizing your workflow
even further. You can learn more in the ``torch.utils.data`` docs
`here <https://pytorch.org/docs/stable/data.html>`__.



.. code-block:: default


    # %%%%%%RUNNABLE_CODE_REMOVED%%%%%%

.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.000 seconds)


.. _sphx_glr_download_recipes_recipes_custom_dataset_transforms_loader.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download

     :download:`Download Python source code: custom_dataset_transforms_loader.py <custom_dataset_transforms_loader.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: custom_dataset_transforms_loader.ipynb <custom_dataset_transforms_loader.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.readthedocs.io>`_
